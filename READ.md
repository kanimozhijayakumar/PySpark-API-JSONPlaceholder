# PySpark API ETL Project (JSONPlaceholder)

## ğŸ“Œ Overview
This project demonstrates how to build an end-to-end ETL pipeline using PySpark by consuming REST APIs.
It focuses on distributed data processing, parallel API ingestion, and handling nested JSON data.

The project is implemented and executed entirely on Google Colab.

## ğŸš€ Key Features
- REST API data extraction using Python
- Parallel API calls using PySpark RDDs
- Conversion of JSON responses into Spark DataFrames
- Handling and flattening nested JSON structures
- Writing processed data to storage
- Fully runnable on Google Colab

## ğŸ›  Tech Stack
- Python
- PySpark
- REST APIs
- Google Colab
- GitHub

## ğŸ”— Data Source
JSONPlaceholder (Fake REST API for testing)  
https://jsonplaceholder.typicode.com

## â–¶ï¸ Run the Project
Click the button below to run the notebook directly in Google Colab:

[![Open In Colab]
(https://colab.research.google.com/assets/colab-badge.svg)]
(https://colab.research.google.com/github/YOUR_USERNAME/pyspark-api-etl-project/blob/main/jsonplaceholder_pyspark.ipynb)

## ğŸ“‚ Project Structure
- `jsonplaceholder_pyspark.ipynb` â€“ Main PySpark notebook
- `README.md` â€“ Project documentation

## ğŸ¯ Learning Outcomes
- Understanding API-based data ingestion
- Implementing parallel processing using PySpark
- Working with semi-structured JSON data
- Applying ETL concepts in a distributed environment

## ğŸ“Œ Use Case
This project simulates real-world data engineering scenarios where data is ingested from APIs and processed using distributed systems.
